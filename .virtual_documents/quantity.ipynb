import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import kagglehub


from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline


from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor

from sklearn.model_selection import cross_validate
from sklearn.metrics import r2_score, mean_squared_error


from sklearn.cluster import KMeans
from statsmodels.tools.tools import add_constant
from statsmodels.stats.outliers_influence import variance_inflation_factor



path = kagglehub.dataset_download("krishangupta33/pharmaceutical-company-wholesale-retail-data")
print("Path to dataset files:", path)


files = os.listdir(path)
print("Files Folder:", files)


file_path=os.path.join(path,"pharma-data.csv")
df=pd.read_csv(file_path)
df.head()





df.drop_duplicates(inplace=True)


df['Product Name'].nunique()


print("Missing Values:", df.isnull().sum())


#drop high cardinality cols
to_drop=['Customer Name','Name of Sales Rep','Manager','Sales Team','City']
df.drop(columns=to_drop,inplace=True)
df.head()


plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()




def preprocess_dataframe(df):
    df_modify = df.copy()


    #replace infinities with NaN
    df_modify.replace([np.inf, -np.inf], np.nan, inplace=True)

    # Cap outliers
    def cap_outliers(column):
        Q1 = df_modify[column].quantile(0.25)
        Q3 = df_modify[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        before_outliers = ((df_modify[column] < lower_bound) | (df_modify[column] > upper_bound)).sum()
        df_modify[column] = np.clip(df_modify[column], lower_bound, upper_bound)
        print(f"{column}: capped {before_outliers} outliers")

    for col in ['Quantity', 'Price', 'Sales']:
        if col in df_modify.columns:
            cap_outliers(col)

    # Create new features
    df_modify["Sales_per_Unit"]=df_modify.apply(
        lambda row:row['Sales']/row['Quantity'] if row['Quantity']>0 else np.nan , axis=1)
    df_modify['Sales']=df_modify['Sales'].clip(lower=1e-5)
    df_modify['Log_Sales']=np.log1p(df_modify['Sales'])

    month_mapping={
        "January": 1, "February": 2, "March": 3, "April": 4,
        "May": 5, "June": 6, "July": 7, "August": 8,
        "September": 9, "October": 10, "November": 11, "December": 12
    }
    df_modify['Month'] = df_modify['Month'].map(month_mapping)
    df_modify['Month_sin'] = np.sin(2 * np.pi * df_modify['Month'] / 12)
    df_modify['Month_cos'] = np.cos(2 * np.pi * df_modify['Month'] / 12)
    # df_modify['Year'] = df_modify['Year'] - df_modify['Year'].min()

    # Step 4: Drop rows with missing values in critical columns
    df_modify.dropna(subset=[
        'Quantity', 'Price', 'Sales', 'Sales_per_Unit',
        'Log_Sales', 'Month_sin', 'Month_cos', 'Year'
    ], inplace=True)

    # scaler = StandardScaler()
    # df_modify[['Latitude', 'Longitude']] = scaler.fit_transform(df_modify[['Latitude', 'Longitude']])

    coords = df_modify[['Latitude', 'Longitude']]
    kmeans = KMeans(n_clusters=5, random_state=42)
    df_modify['Region'] = kmeans.fit_predict(coords)

    df_modify= pd.get_dummies(df_modify, columns=['Region'], drop_first=True)

    vif_cols = ['Quantity', 'Price', 'Sales', 'Sales_per_Unit', 'Log_Sales', 'Month_sin', 'Month_cos', 'Year']
    X = add_constant(df_modify[vif_cols])
    vif_data = pd.DataFrame({
        "Feature": X.columns,
        "VIF": [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    })
    df_modify.head(2)




    return df_modify,vif_data

